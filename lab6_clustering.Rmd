---
title: "lab 6"
author: "Grace Bianchi"
date: "2023-02-16"
output: 
  html_document:
    code_folding: show
  
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(here)
library(janitor)
library(palmerpenguins)

### packages for cluster analysis
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```


# Intro to Cluster analysis - k-means, hierachical

## Part 1: K-means clustering

```{r data exploration}
ggplot(data = penguins) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex),
             size = 3,
             alpha = 0.7) + ## transparency
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))

ggplot(penguins) +
  geom_point(aes(x = flipper_length_mm, 
                 y = body_mass_g,
                 color = species,
                 shape = sex),
             size = 3, alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))
```

### Create a complete (remove NAs), scaled version of the data 

```{r}
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm) 

penguins_scale <- penguins_complete %>% 
  select(ends_with("_mm"), body_mass_g) %>% 
  scale()
```

```{r}
# looks at 25 different indices and more (n = 11) suggested 3 is the best number of clusters
number_est <- NbClust(penguins_scale, min.nc = 2, max.nc = 10, method = "kmeans")

fviz_nbclust(penguins_scale, 
             FUNcluster = kmeans, 
             method = 'wss', # minimize sum of squares
             k.max = 10)
## look for where it starts to level off, when the number of clusters does not add value 
## don't want to go past 5

```

### run some k-means clusterings

```{r}
# starts with random number of set centroids
set.seed(123)

# kmeans is included in r
penguins_km <- kmeans(penguins_scale,
                      centers = 3, # number of centers, iterates until reaches stable point
                      iter.max = 10, # 
                      nstart = 25) # out of 25 times ,identify number with lowest sum of squares of clusters


# penguins_km$size 
# penguins$cluster 

penguins_cl <- penguins_complete %>% 
  mutate(cluster_no = factor(penguins_km$cluster)) # factors identifies clusters as categorical

```


```{r}
ggplot(penguins_cl) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g, 
                 color = cluster_no,
                 shape = species))
# comparing cluster groups compared to actual data

ggplot(penguins_cl) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm, 
                 color = cluster_no,
                 shape = species)) +
  scale_color_manual(values = c("cornflowerblue", "palevioletred1", "#bCDD88"))

penguins_cl %>% 
  select(species, cluster_no) %>% 
  table() # all Gentoos were in their own cluster
# using known species, if clusters have some physical meaning to take away


```

## Hierarchical

### Start with complete linkage

Joins two ind. observations into clusters, before joining clusters, it looks at maximum distance

```{r}
### create distance matrix
peng_dist <- dist(penguins_scale, method = 'euclidean')

### hierarchical clustering (complete linkage)
peng_hc_complete <- hclust(peng_dist, method = 'complete')
### also: single, average, ward.D

### plot a dendrogram
plot(peng_hc_complete, cex = .6, hang = -1)

### cut the tree into three clusters
peng_cut_hc <- cutree(peng_hc_complete, 3)
### compare clustering to see how they match up
table(peng_cut_hc, penguins_complete$species)

```

Classify

Binary logistic regression vs. Clustering
**BLR:** supervised machine learning; have known set of data with characteristics
- trying to predict on new data based on known set of data

Clusters: unsupervised
When using clusters, species is not known before, but it uses data to group species into clusters


## World Bank data: read in and simplify


```{r}
wb_env <- read_csv(here("data", "wb_env.csv"))

wb_ghg_20 <- wb_env %>% 
  slice_max(n = 20, ghg)


wb_scaled <- wb_ghg_20 %>% 
  select(where(is.numeric)) %>% 
  scale()

# since we can't have categorical data in the clustering, making the country the row name makes it easier when we visualize
rownames(wb_scaled) <- wb_ghg_20$name 
```


### Find euclidean distances

```{r}
euc_distance <- dist(wb_scaled, method = 'euclidean')
```

### Perform hierarchical clustering with complete linkage

```{r}
hc_complete <- hclust(euc_distance, method = "complete")

plot(hc_complete, cex = .6, hang = -1)
```

### Perform hierarchical clustering by single linkage

```{r}
hc_single<- hclust(euc_distance, method = "single")
plot(hc_single, cex = .6, hang = -1)

```

### Make a tanglegram

Compares two dendrograms- how different is clustering between hierarchical and single linkage
- Poland and italy are in their own cluster in both methods
- solid line - similar bw two groups
- dashed line - significant difference


```{r}
dend_complete <- as.dendrogram(hc_complete)
dend_single <- as.dendrogram(hc_single)

tanglegram(dend_complete, dend_single)

# entanglement- how messy are two dendrograms compared to each other
entanglement(dend_complete, dend_single)

# reorganizes clusters
untangle(dend_complete, dend_single, method = "step1side") %>% 
  entanglement() # smaller entanglement, changes how thiings are lined up to see differences between the two

untangle(dend_complete, dend_single, method = "step1side") %>% 
  tanglegram(common_subtrees_color_branches = TRUE)

```


### Let's make a dendrogram in ggplot!

```{r}
ggdendrogram(hc_complete, rotate = TRUE) +
  theme_minimal() +
  labs(x = "Country", y = "Euclidean Distance")
```

