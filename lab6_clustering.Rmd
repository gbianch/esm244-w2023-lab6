---
title: "lab 6"
author: "Grace Bianchi"
date: "2023-02-16"
output: 
  html_document:
    code_folding: show
  
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE, echo = TRUE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(here)
library(janitor)
library(palmerpenguins)

### packages for cluster analysis
library(NbClust)
library(cluster)
library(factoextra)
library(dendextend)
library(ggdendro)
```


# Intro to Cluster analysis - k-means, hierachical

## Part 1: K-means clustering

```{r data exploration}
ggplot(data = penguins) +
  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species, shape = sex),
             size = 3,
             alpha = 0.7) + ## transparency
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))

ggplot(penguins) +
  geom_point(aes(x = flipper_length_mm, 
                 y = body_mass_g,
                 color = species,
                 shape = sex),
             size = 3, alpha = 0.7) +
  scale_color_manual(values = c("orange", "cyan4", "darkmagenta"))
```

### Create a complete (remove NAs), scaled version of the data 

```{r}
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm) 

penguins_scale <- penguins_complete %>% 
  select(ends_with("_mm"), body_mass_g) %>% 
  scale()
```

```{r}
# looks at 25 different indices and more (n = 11) suggested 3 is the best number of clusters
number_est <- NbClust(penguins_scale, min.nc = 2, max.nc = 10, method = "kmeans")

fviz_nbclust(penguins_scale, 
             FUNcluster = kmeans, 
             method = 'wss', # minimize sum of squares
             k.max = 10)
## look for where it starts to level off, when the number of clusters does not add value 
## don't want to go past 5

```

### run some k-means clusterings

```{r}
# starts with random number of set centroids
set.seed(123)

# kmeans is included in r
penguins_km <- kmeans(penguins_scale,
                      centers = 3, # number of centers, iterates until reaches stable point
                      iter.max = 10, # 
                      nstart = 25) # out of 25 times ,identify number with lowest sum of squares of clusters


# penguins_km$size 
# penguins$cluster 

penguins_cl <- penguins_complete %>% 
  mutate(cluster_no = factor(penguins_km$cluster)) # factors identifies clusters as categorical

```


```{r}
ggplot(penguins_cl) +
  geom_point(aes(x = flipper_length_mm,
                 y = body_mass_g, 
                 color = cluster_no,
                 shape = species))
# comparing cluster groups compared to actual data

ggplot(penguins_cl) +
  geom_point(aes(x = bill_length_mm,
                 y = bill_depth_mm, 
                 color = cluster_no,
                 shape = species)) +
  scale_color_manual(values = c("cornflowerblue", "palevioletred1", "#bCDD88"))

penguins_cl %>% 
  select(species, cluster_no) %>% 
  table() # all Gentoos were in their own cluster
# using known species, if clusters have some physical meaning to take away


```

## Hierarchical

### Start with complete linkage

Joins two ind. observations into clusters, before joining clusters, it looks at maximum distance

```{r}
### create distance matrix
peng_dist <- dist(penguins_scale, method = 'euclidean')

### hierarchical clustering (complete linkage)
peng_hc_complete <- hclust(peng_dist, method = 'complete')
### also: single, average, ward.D

### plot a dendrogram
plot(peng_hc_complete, cex = .6, hang = -1)

### cut the tree into three clusters
peng_cut_hc <- cutree(peng_hc_complete, 3)
### compare clustering to see how they match up
table(peng_cut_hc, penguins_complete$species)

```

Classify

Binary logistic regression vs. Clustering
**BLR:** supervised machine learning; have known set of data with characteristics
- trying to predict on new data based on known set of data

Clusters: unsupervised
When using clusters, species is not known before, but it uses data to group species into clusters



```{r}

```

